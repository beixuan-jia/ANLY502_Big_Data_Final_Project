{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-66-237.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe2ac688710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3://miaowang1009/ANLY502_Final/df.parquet/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = df.select(df.AvgTone, df.NumArticles,df.NumMentions, df.ActionGeo_Type, df.EventRootCode, df.QuadClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AvgTone: float (nullable = true)\n",
      " |-- NumArticles: integer (nullable = true)\n",
      " |-- NumMentions: integer (nullable = true)\n",
      " |-- ActionGeo_Type: string (nullable = true)\n",
      " |-- EventRootCode: string (nullable = true)\n",
      " |-- QuadClass: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_feat.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+--------------+-------------+---------+----------+----------+\n",
      "|    AvgTone|NumArticles|NumMentions|ActionGeo_Type|EventRootCode|QuadClass|AvgToneBin|AvgToneTag|\n",
      "+-----------+-----------+-----------+--------------+-------------+---------+----------+----------+\n",
      "|  0.7025761|         10|         10|             4|           04|        1|       1.0|  Positive|\n",
      "|-0.73937154|          6|          6|             5|           01|        1|       0.0|   Negtive|\n",
      "|-0.73937154|          2|          2|             4|           01|        1|       0.0|   Negtive|\n",
      "|-0.73937154|          2|          2|             1|           04|        1|       0.0|   Negtive|\n",
      "|  0.7025761|          2|          2|             4|           04|        1|       1.0|  Positive|\n",
      "|-0.73937154|          6|          6|             4|           03|        1|       0.0|   Negtive|\n",
      "|  0.7025761|         10|         10|             4|           04|        1|       1.0|  Positive|\n",
      "| -2.1311476|          3|          3|             4|           17|        4|       0.0|   Negtive|\n",
      "| -2.1311476|          6|          6|             4|           17|        4|       0.0|   Negtive|\n",
      "| -2.1311476|          1|          1|             4|           17|        4|       0.0|   Negtive|\n",
      "| -3.4739454|        180|        180|             4|           01|        1|       0.0|   Negtive|\n",
      "|  -7.402101|        965|        995|             0|           01|        1|       0.0|   Negtive|\n",
      "| -6.5946918|         14|         14|             1|           13|        3|       0.0|   Negtive|\n",
      "| -2.2292848|        136|        136|             2|           12|        3|       0.0|   Negtive|\n",
      "|  3.3175356|          5|          5|             4|           03|        1|       1.0|  Positive|\n",
      "|  3.3175356|          3|          3|             4|           03|        1|       1.0|  Positive|\n",
      "| -2.7027028|          5|          5|             3|           07|        2|       0.0|   Negtive|\n",
      "|  -4.130409|         19|         19|             4|           02|        1|       0.0|   Negtive|\n",
      "|  -4.130409|         39|         39|             3|           02|        1|       0.0|   Negtive|\n",
      "| -2.1802325|          2|          2|             4|           02|        1|       0.0|   Negtive|\n",
      "+-----------+-----------+-----------+--------------+-------------+---------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create add new column to the dataset\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "bucketizer = Bucketizer(splits=[ -100, 0, 100, float('Inf') ],inputCol=\"AvgTone\", outputCol=\"AvgToneBin\")\n",
    "df_feat = bucketizer.setHandleInvalid(\"keep\").transform(df_feat)\n",
    "\n",
    "#df_buck.show()\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "t = {0.0:\"Negtive\", 1.0:\"Positive\"}\n",
    "udf_foo = udf(lambda x: t[x], StringType())\n",
    "df_feat = df_feat.withColumn(\"AvgToneTag\", udf_foo(\"AvgToneBin\"))\n",
    "df_feat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_feat.drop('AvgTone').drop('AvgToneTag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NumArticles',\n",
       " 'NumMentions',\n",
       " 'ActionGeo_Type',\n",
       " 'EventRootCode',\n",
       " 'QuadClass',\n",
       " 'AvgToneBin']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NumArticles: integer (nullable = true)\n",
      " |-- NumMentions: integer (nullable = true)\n",
      " |-- ActionGeo_Type: string (nullable = true)\n",
      " |-- EventRootCode: string (nullable = true)\n",
      " |-- QuadClass: string (nullable = true)\n",
      " |-- AvgToneBin: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import (DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LogisticRegression)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting categorical features\n",
    "categorical_columns = ['ActionGeo_Type', 'EventRootCode', 'QuadClass']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In order to One-Hot encode this categorical features we will first pass them through an indexer and then to an encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The index of string values multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "    for c in categorical_columns]\n",
    "# The encode of indexed values multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, we will join the categorical encoded features with the numerical ones and make a vector with both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing encoded values\n",
    "categorical_encoded = [encoder.getOutputCol() for encoder in encoders]\n",
    "numerical_columns = ['NumArticles', 'NumMentions']\n",
    "inputcols = categorical_encoded + numerical_columns\n",
    "assembler = VectorAssembler(inputCols=inputcols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[NumArticles: int, NumMentions: int, ActionGeo_Type: string, EventRootCode: string, QuadClass: string, AvgToneBin: double, ActionGeo_Type_indexed: double, EventRootCode_indexed: double, QuadClass_indexed: double, ActionGeo_Type_indexed_encoded: vector, EventRootCode_indexed_encoded: vector, QuadClass_indexed_encoded: vector, features: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Row(NumArticles=10, NumMentions=10, ActionGeo_Type='4', EventRootCode='04', QuadClass='1', AvgToneBin=1.0, ActionGeo_Type_indexed=0.0, EventRootCode_indexed=0.0, QuadClass_indexed=0.0, ActionGeo_Type_indexed_encoded=SparseVector(6, {0: 1.0}), EventRootCode_indexed_encoded=SparseVector(21, {0: 1.0}), QuadClass_indexed_encoded=SparseVector(4, {0: 1.0}), features=SparseVector(33, {0: 1.0, 6: 1.0, 27: 1.0, 31: 10.0, 32: 10.0}))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we will set up a pipeline to automatize this stages.\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders+[assembler])\n",
    "model = pipeline.fit(df_features)\n",
    "# Transform data\n",
    "transformed = model.transform(df_features)\n",
    "display(transformed)\n",
    "transformed.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "final_data = transformed.select('features', 'AvgToneBin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classification models\n",
    "dtc = DecisionTreeClassifier(labelCol='AvgToneBin', featuresCol='features')\n",
    "#rfc = RandomForestClassifier(numTrees=50, labelCol='AvgToneBi', featuresCol='features')\n",
    "#gbt = GBTClassifier(labelCol='AvgToneBi', featuresCol='features', maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will perform a classic 80/20 split between training and testing data.\n",
    "train_data, test_data = final_data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Models\n",
    "dtc_model = dtc.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining Predictions\n",
    "dtc_preds = dtc_model.transform(test_data)\n",
    "#rfc_preds = rfc_model.transform(test_data)\n",
    "#gbt_preds = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Modelâ€™s Performance\n",
    "## Our evaluator will be the ROC. We will initialize its class and pass it the predicitons in order to obtain the value.\n",
    "my_eval = BinaryClassificationEvaluator(labelCol='AvgToneBin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC\n",
      "0.5057174142351178\n"
     ]
    }
   ],
   "source": [
    "# Display Decision Tree evaluation metric\n",
    "print('DTC')\n",
    "print(my_eval.evaluate(dtc_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_model.write().save('s3://miaowang1009/ANLY502_Final/model_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_18396dd71f57) of depth 5 with 11 nodes is not an estimator instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-5d7ed866e9ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mfilled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mspecial_characters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'6'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'7'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'9'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'10'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 class_names = ['0','1','2','3','4','5','6','7','8','9','10'])\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/sklearn/tree/_export.py\u001b[0m in \u001b[0;36mexport_graphviz\u001b[0;34m(decision_tree, out_file, max_depth, feature_names, class_names, label, filled, leaves_parallel, impurity, node_ids, proportion, rotate, rounded, special_characters, precision)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \"\"\"\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m     \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m     \u001b[0mown_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0mreturn_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not an estimator instance.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mattributes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: DecisionTreeClassificationModel (uid=DecisionTreeClassifier_18396dd71f57) of depth 5 with 11 nodes is not an estimator instance."
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(dtc_model, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True, feature_names=['0','1','2','3','4','5','6','7','8','9','10'], \n",
    "                class_names = ['0','1','2','3','4','5','6','7','8','9','10'])\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "#graph05.write_png(\"dtree_5_best.png\")\n",
    "Image(graph.create_png())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
